{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Arrhythmia Classification from Photoplethysmography Signals based on VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import operator as op #\n",
    "import random\n",
    "import math\n",
    "from scipy import signal as sig\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=8)\n",
    "parser.add_argument('--multiLabel', type=bool, default=False, help='enable multiple label')\n",
    "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
    "parser.add_argument('--recordLength', type=int, default=2500, help='the length of input record')\n",
    "parser.add_argument('--niter', type=int, default=200, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.9, help='beta1 for adam. ')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--outf', default='.', help='folder to output model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "opt = parser.parse_args(['--dataroot','./PPGArrhythmiaClassification/','--cuda'])\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "cudnn.benchmark = True\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean filtering function\n",
    "def MeanFilter(ppg,M):\n",
    "    if M%2 ==0:\n",
    "        M = M+1;\n",
    "    Len = math.floor((M-1)/2)\n",
    "    \n",
    "    x = np.zeros((1,Len+len(ppg)+Len))\n",
    "    x = x[0,:]\n",
    "    x[0:Len] = ppg[0:Len]\n",
    "    x[Len:Len+len(ppg)] = ppg\n",
    "    x[Len+len(ppg):len(x)] = ppg[-Len:,]\n",
    "    \n",
    "    ppgs = ppg.astype(float)\n",
    "    for i in range(3,len(x)-Len):\n",
    "        ppgs[i-Len] = np.mean(x[i-Len:i+Len])\n",
    "    return ppgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dir, ext='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dir (string): Directory\n",
    "            ext (string): the extension used to claim training or testing data\n",
    "\n",
    "        \"\"\"\n",
    "        labels = np.zeros((1,1))\n",
    "        ppgseg = np.zeros((1,1000))\n",
    "        with open(ext+'_list.txt','r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                path1 = os.path.join('./NewPPGData1103',line[1:-1])\n",
    "                path2 = os.path.join('./NewPPGData1207',line[1:-1])\n",
    "                if os.path.exists(path1):\n",
    "                    data = sio.loadmat(path1)\n",
    "                else:\n",
    "                    data = sio.loadmat(path2)\n",
    "                labels = np.vstack((labels,data['labels']))\n",
    "                ppgseg = np.vstack((ppgseg,data['ppgseg']))       \n",
    "        \n",
    "        labels = labels[1:len(labels),:]\n",
    "        ppgseg = ppgseg[1:len(ppgseg),:]\n",
    "        # labels\n",
    "        # 0 sinus rhythm\n",
    "        # 1 premature ventricular contraction\n",
    "        # 2 premature atrial contraction\n",
    "        # 3 ventricular tachycardia\n",
    "        # 4 supraventricular tachycardia\n",
    "        # 5 atrial fibrillation.\n",
    "        print(ppgseg.shape)\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.ppgseg = ppgseg\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ppgseg = self.ppgseg[idx]\n",
    "        labels = self.labels[idx]\n",
    "    \n",
    "        sample = { 'ppgseg': ppgseg, 'labels': labels}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class CropAverage(object):\n",
    "    \"\"\"\n",
    "    Crop the signal to certain length, and provide the average sbp as the target\n",
    "\n",
    "    Args:\n",
    "        length (int): the length to be cropped to\n",
    "    \"\"\"\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        ppgseg, labels= sample['ppgseg'], sample['labels']\n",
    "        signal = np.array([ppgseg])\n",
    "\n",
    "        return {'signal': signal, 'labels': labels}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        signal = sample['signal']\n",
    "        labels = sample['labels']\n",
    "\n",
    "        return {'signal': torch.from_numpy(signal).float(),\n",
    "                'labels': torch.from_numpy(labels).float()\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading training set\n",
    "train_dataset = MyDataset(dir=opt.dataroot,\n",
    "                                 ext='train',\n",
    "                                 transform=transforms.Compose([\n",
    "                                     CropAverage(opt.recordLength),\n",
    "                                     ToTensor()\n",
    "                                 ]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batchSize,\n",
    "                          shuffle=True,  num_workers=int(opt.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading validation set\n",
    "valid_dataset = MyDataset(dir=opt.dataroot,\n",
    "                                 ext='valid',\n",
    "                                 transform=transforms.Compose([\n",
    "                                     CropAverage(opt.recordLength),\n",
    "                                     ToTensor()\n",
    "                                 ]))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=opt.batchSize,\n",
    "                          shuffle=False,  num_workers=int(opt.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, ngpu, num_classes=4, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256), \n",
    "            nn.ReLU(True),        \n",
    "            nn.Dropout(),        \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            x = nn.parallel.data_parallel(self.features, x, range(self.ngpu))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = nn.parallel.data_parallel(self.classifier, x, range(self.ngpu))\n",
    "        else:\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d): \n",
    "                nn.init.kaiming_normal_(m.weight.data,mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data,mode='fan_out')\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "def make_layers(cfg, batch_norm=True):\n",
    "    layers = []\n",
    "    in_channels = 1 \n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool1d(kernel_size=3, stride=3)]\n",
    "        else:\n",
    "            conv1d = nn.Conv1d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv1d, nn.BatchNorm1d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv1d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [32, 32, 'M', 64, 64, 'M', 128, 128, 128, 'M', 256, 256, 256, 'M', 256, 256, 256, 'M'],\n",
    "    'E': [32, 32, 'M', 64, 64, 'M', 128, 128, 128, 128, 'M', 256, 256, 256, 256, 'M', 256, 256, 256, 256, 'M'],\n",
    "}\n",
    "\n",
    "def vgg16_bn(**kwargs):\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    \"\"\"\n",
    "    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngpu = int(opt.ngpu)\n",
    "num_types = 1\n",
    "model = vgg16_bn(ngpu=ngpu,num_classes=6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngpu = int(opt.ngpu)\n",
    "in_x = Variable(torch.randn(20,1,500*2))\n",
    "in_y = Variable(torch.randn(20,1))\n",
    "out_x = model(in_x)\n",
    "#fc_fea = resnet_cifar(model, in_x)\n",
    "#print(fc_fea.shape)\n",
    "print(in_x.shape)\n",
    "print(out_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()\n",
    "if opt.cuda:\n",
    "    model.cuda()\n",
    "    loss_fun.cuda()\n",
    "optimizer = optim.Adam(list(model.parameters()), lr = opt.lr, betas=(opt.beta1, 0.999),weight_decay=0.004)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(opt.niter):\n",
    "    # Model training\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    labelsall = []\n",
    "    model.train()\n",
    "    for i, train_data in enumerate(train_loader, 0):\n",
    "        model.zero_grad()\n",
    "        inputs, labels = train_data['signal'], train_data['labels']\n",
    "        if opt.cuda:\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "                   \n",
    "        output = model(inputs)\n",
    "        labels = labels[:,0].long()\n",
    "        loss = loss_fun(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())  \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        predicted = predicted.cpu()\n",
    "        labels = labels.cpu()\n",
    "        predictions.extend(predicted.numpy())\n",
    "        labelsall.extend(labels.numpy())\n",
    "        \n",
    "    scheduler.step()\n",
    "    print('[%d/%d] Loss: %.4f Training Accuracy: %.4f' %(epoch, opt.niter, np.average(losses), \n",
    "                                                         classification.accuracy_score(labelsall, predictions))) \n",
    "    EpochAcc = np.vstack((EpochAcc,np.array((epoch, np.average(losses)))))\n",
    "    # Model validation\n",
    "    model.eval()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        # validate\n",
    "        predictions = []\n",
    "        labelsall = []\n",
    "        losses = []\n",
    "        for i, valid_data in enumerate(valid_loader, 0):\n",
    "            inputs, labels = valid_data['signal'], valid_data['labels']\n",
    "            if opt.cuda:\n",
    "                inputs = Variable(inputs).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "                \n",
    "            output = model(inputs)\n",
    "            labels = labels[:,0].long()\n",
    "            loss = loss_fun(output, labels)\n",
    "            losses.append(loss.item())  \n",
    "            _,predicted = torch.max(output.data, 1)\n",
    "            predicted = predicted.cpu()\n",
    "            labels = labels.cpu()\n",
    "            \n",
    "            predictions.extend(predicted.numpy())\n",
    "            labelsall.extend(labels.numpy())\n",
    "        print('[%d/%d] Validation Accuracy: %.4f' %(epoch, opt.niter, classification.accuracy_score(labelsall, predictions)))\n",
    "        EpochValidAcc = np.vstack((EpochValidAcc,np.array((epoch, np.average(losses)))))\n",
    "        \n",
    "        if classification.accuracy_score(labelsall, predictions) > ValidAcc:\n",
    "            ValidAcc = classification.accuracy_score(labelsall, predictions)\n",
    "            labelsAll = labelsall\n",
    "            predictionsAll = predictions\n",
    "            torch.save(model, 'modelPPG.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test set\n",
    "test_dataset = MyDataset(dir=opt.dataroot,\n",
    "                                 ext='test',\n",
    "                                 transform=transforms.Compose([\n",
    "                                     CropAverage(opt.recordLength),\n",
    "                                     ToTensor()\n",
    "                                 ]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=opt.batchSize,\n",
    "                          shuffle=False,  num_workers=int(opt.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('modelPPG.pkl')\n",
    "predictions = []\n",
    "labelsall = []\n",
    "Fea_all = np.zeros((1,256), dtype = float)\n",
    "pred_score = np.zeros((1,6), dtype = float)\n",
    "for i, test_data in enumerate(test_loader, 0):\n",
    "    inputs, labels = test_data['signal'], test_data['labels']\n",
    "    if opt.cuda:\n",
    "        inputs = Variable(inputs).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        \n",
    "    output = model(inputs)\n",
    "    labels = labels[:,0].long()\n",
    "    score = output.data.cpu().numpy()\n",
    "    pred_score = np.vstack((pred_score,score))       \n",
    "\n",
    "    _,predicted = torch.max(output.data, 1)\n",
    "    predicted = predicted.cpu()\n",
    "    labels = labels.cpu()  \n",
    "    predictions.extend(predicted.numpy())\n",
    "    labelsall.extend(labels.numpy())\n",
    "    \n",
    "labelsAll = labelsall\n",
    "predictionsAll = predictions\n",
    "pred_score = pred_score[1:len(pred_score),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification.classification_report(labelsAll,predictionsAll,digits=4))\n",
    "confusion = classification.confusion_matrix(labelsAll,predictionsAll)\n",
    "classes = ['SR', 'PVC', 'PAC','VT','SVT','AF']\n",
    "indices = range(len(confusion))\n",
    "plt.xticks(indices, classes)\n",
    "plt.yticks(indices, classes)\n",
    "plt.imshow(confusion,cmap=plt.cm.OrRd)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "for first_index in range(len(confusion)):\n",
    "    for second_index in range(len(confusion[first_index])):\n",
    "        plt.text(first_index-0.25, second_index, confusion[second_index][first_index])\n",
    "        \n",
    "print('Sensitivity, Specificity, Positive predictive value,Negative predictive values,Accuracy')\n",
    "for i in range(len(confusion)):\n",
    "    TP = confusion[i,i]\n",
    "    FN = sum(confusion[i,:])-TP\n",
    "    FP = sum(confusion[:,i])-TP\n",
    "    TN = sum(sum(confusion))-TP-FP-FN\n",
    "    \n",
    "    print(np.array([TP/(TP+FN), TN/(FP+TN), TP/(TP+FP), TN/(FN+TN), (TP+TN)/(TP+FP+FN+TN)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "\n",
    "y_true = label_binarize(np.array(labelsAll), classes=[0, 1, 2, 3, 4, 5])\n",
    "n_classes = 6\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], pred_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Average ROC curve (AUC = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linewidth=2)\n",
    "   \n",
    "colors = cycle(['aqua', 'darkorange', 'blue','red','blue','red'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5,\n",
    "             label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
